---
title: "Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance"
collection: publications
permalink: /publication/2021-5-27-QFS
excerpt: ''
date: 2021-5-27
venue: 'Findings of ACL'
paperurl: 'https://arxiv.org/abs/2105.12969'
authors: 'Dan Su*, Tiezheng Yu*, Pascale Fung'
citation: ''
paper: 'https://arxiv.org/abs/2105.12969'
slide:
award:
code: 'https://github.com/HLTCHKUST/QFS'
show_year: true
---
Query focused summarization (QFS) models aim to generate summaries from source documents that can answer the given query. Most previous work on QFS only considers the query relevance criterion when producing the summary. However, studying the effect of answer relevance in the summary generating process is also important. In this paper, we propose QFS-BART, a model that incorporates the explicit answer relevance of the source documents given the query via a question answering model, to generate coherent and answer-related summaries. Furthermore, our model can take advantage of large pre-trained models which improve the summarization performance significantly. Empirical results on the Debatepedia dataset show that the proposed model achieves the new state-of-the-art performance.
[Paper](https://arxiv.org/abs/2105.12969)
[code](https://github.com/HLTCHKUST/QFS)

